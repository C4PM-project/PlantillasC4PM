{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc397ecf",
   "metadata": {},
   "source": [
    "# Analizamos los emails de cada empleado contenidos en el dataset del Instituto\n",
    "Para esto se debe descargar los dataset (correos electrónicos) en formato .mbox, que se puede hacer con thunderbird o desde gmail. Al fichero descargado hay que agregarle en el nombre la extensión .mbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e348c",
   "metadata": {},
   "outputs": [],
   "source": [
    " #--> esto tiene que ejecutarse antes uno a uno en otro notebook\n",
    "#pip install pandas\n",
    "#pip install wordcloud\n",
    "#pip install nltk\n",
    "#pip install sklearn\n",
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd1b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertir el fichero .mbox a maildir para poder manipular mejor el contenido del correo electrónico\n",
    "import mailbox\n",
    "\n",
    "# Indicar la ruta del fichero .mbox \n",
    "mbox_path = \"C:\\\\.......\\\\dataset_file.mbox\"\n",
    "\n",
    "# Indicar la ruta donde se quiere guardar el fichero que se está creando con el contenido de los correos electrónicos del .mbox en el nuevo formato maildir\n",
    "maildir_path = \"C:\\\\.......\\\\dataset_file\"\n",
    "\n",
    "# Abrir el fichero .mbox\n",
    "mbox_file = mailbox.mbox(mbox_path)\n",
    "\n",
    "# Crear el fichero maildir a partir del .mbox\n",
    "maildir_file = mailbox.Maildir(maildir_path)\n",
    "\n",
    "# Agregar cada e-mail del .mbox en el nuevo formato maildir\n",
    "for message in mbox_file:\n",
    "    \n",
    "    maildir_file.add(message)\n",
    "\n",
    "mbox_file.close()\n",
    "maildir_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0816c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Después leer los correos en string para manipular mejor el texto del cuerpo de los mensajes que componen la bandeja de entreda del e-mail, \n",
    "#pues, no saldría con los diferentes tipos de formato multipart que son más complejos de decodificar. \n",
    "\n",
    "import pandas as pd \n",
    "import mailbox\n",
    "\n",
    "\n",
    "maildir_path = \"C:\\\\.......\\\\dataset_file\"\n",
    "maildir_file = mailbox.Maildir(maildir_path)\n",
    "\n",
    "# Crear una lista para guardar cada mensaje\n",
    "data_list = []\n",
    "\n",
    "# Agregar cada mensaje del .maildir en la lista\n",
    "for key, message in maildir_file.items():\n",
    "   \n",
    "    # Extraer el texto de los mensajes en formato string y convertirlo a minúsculas\n",
    "    body = message.as_string().lower()\n",
    "    data_list.append([body])\n",
    "             \n",
    "# Crear un dataFrame con los mensajes de la lista que tendrá una sola columna con nombre Texto\n",
    "df_data = pd.DataFrame(data_list, columns=['Texto'])\n",
    "\n",
    "maildir_file.close()\n",
    "\n",
    "pd.set_option('max_colwidth',1000) #Ampliar el tamaño del dataframe para visualizar mejor su contenido\n",
    "\n",
    "# Imprimir las dos primeras filas del dataframe creado\n",
    "df_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fb24f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c51fe4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertimos el df a .csv para tener un backup\n",
    "df_data.to_csv(\"C:\\\\..............\\\\nombre.csv\", sep= '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa815a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unir todos los mensajes listado en el dataframe para obtener un solo texto con más palabras que analizar\n",
    "df_full= df_data.agg('sum')\n",
    "df = pd.DataFrame([df_full], columns=['Texto'])\n",
    "#Imprimir el dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e442bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "#Crear una función para realizar las tareas de limpieza de datos, cuantas veces sea necesario, sobre el texto del dataframe\n",
    "def clean_str1(text):\n",
    "      \n",
    "    text = re.sub('(?P<url>https?://[^\\s]+)', ' 8888 ', text) #quitar URL's\n",
    "    text = re.sub(r'[\\w\\-][\\w\\-\\.]+@[\\w\\-][\\w\\-\\.]+[a-zA-Z]{1,4}', ' 9999 ', text) #quitar direcciones de email's\n",
    "    text = re.sub('</div>', ' ', text)  #buscar las palabras </div> en todo el texto y reemplazarlas por un espacio en blanco\n",
    "    text = re.sub('\\n',' ', text) #esto es para reemplazar los \\n por nada de espacio\n",
    "    text = re.sub('=c3=a1', 'a', text) #buscar =c3=a1 y reemplazarlo por la letra \"a\" - aunque en UTF-8 sería \"á\",\n",
    "    text = re.sub('=c3=a9', 'e', text) # pero se le ha quitado la tilde a todos estos caracteres para procesar mejor el texto -\n",
    "    text = re.sub('=c3=ad', 'i', text)\n",
    "    text = re.sub('=c3=b3', 'o', text)\n",
    "    text = re.sub('=c3=ba', 'u', text)\n",
    "    text = re.sub('=c3=b1', 'ñ', text)\n",
    "    text = re.sub('  +', ' ', text)  #quitar espacios en blanco\n",
    "    text = re.sub(r'(?<=\\w)</strong>(?=\\w)',' ', text)  #quitar caracteres específicos en medio de una palabra\n",
    "    text = re.sub(r'(?<=\\w)\\n(?=\\w)',' ', text)\n",
    "    text = re.sub('  +', ' ', text)  #quitar espacios en blanco\n",
    "    text = re.sub(r'(?<=\\w)=\\n(?=\\w)',' ', text)\n",
    "   \n",
    "      \n",
    "    return text.strip().lower()\n",
    "\n",
    "clean1 = lambda x: clean_str1(x)\n",
    "\n",
    "# Aplicar los cambios y guardarlos en un nuevo dataframe\n",
    "df1 = pd.DataFrame(df.Texto.apply(clean1).dropna()) \n",
    "#Imprimir el dataframe\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c93c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar las librerías necesarias para analizar textos\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b3a77ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_str2(text):\n",
    "\n",
    "    text = re.sub(\"\\b*\\d\\b*\", \"\", text) ##quita los dígitos sueltos y que están entre palabras\n",
    "    text = re.sub(r\"\\b\\w\\b\", \" \", text) ##quita las letras sueltas\n",
    "    text = re.sub(' +', ' ', text)  #quita espacios en blanco entre palabras\n",
    "    text = re.sub(r\"\\b\\w\\b\", \" \", text) ##quita las letras sueltas\n",
    "    text = re.sub(' +', ' ', text)  #quita espacios en blanco entre palabras\n",
    "    text = re.sub(r\"\\b\\w\\b\", \" \", text) ##quita las letras sueltas\n",
    "    text = re.sub('=c3=a1', 'a', text)\n",
    "    text = re.sub('=c3=a9', 'e', text)\n",
    "    text = re.sub('=c3=ad', 'i', text)\n",
    "    text = re.sub('=c3=b3', 'o', text)\n",
    "    text = re.sub('=c3=ba', 'u', text)\n",
    "    text = re.sub('=c3=b1', 'ñ', text)#buscar =c3=b1 y reemplazarlo por la letra \"ñ\" - decodificando caracteres especiales en UTF-8-\n",
    "    text = re.sub(' +', ' ', text)  #quita espacios en blanco entre palabras ok\n",
    "    text = re.sub(r\"[%s]\" % re.escape(string.punctuation), \" \", text) # Quitar todos los signos de puntuación\n",
    "    text = re.sub(' +', ' ', text)  #quita espacios en blanco entre palabras\n",
    "\n",
    "    return text.strip().lower()\n",
    "\n",
    "clean2 = lambda x: clean_str2(x)\n",
    "\n",
    "df2 = pd.DataFrame(df1.Texto.apply(clean2).dropna()) \n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5a05a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitar las palabras en inglés definidas en la lista stop_words de python\n",
    "stop_words = stopwords.words('english')\n",
    "df2['Texto'] = df2['Texto'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e92ec5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitar las palabras en español definidas en la lista stop_words de python\n",
    "stop_words = stopwords.words('spanish')\n",
    "df2['Texto'] = df2['Texto'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07265a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0866034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_str3(text):\n",
    "\n",
    "    text = re.sub(r'\\b\\w{2}\\b', '', text) #quita las palabras con dos letras\n",
    "    text = re.sub(r'\\b\\w{3}\\b', '', text) #quita las palabras con tres letras\n",
    "    text = re.sub(' +', ' ', text)  #quita espacios en blanco entre palabras\n",
    "\n",
    "    return text.strip().lower()\n",
    "\n",
    "clean3 = lambda x: clean_str3(x)\n",
    "\n",
    "df3 = pd.DataFrame(df2.Texto.apply(clean3).dropna()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8515ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f1e53f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A medida que se imprimen los resultados de aplicar las funciones de limpieza y stop_words sobre el texto del dataframe\n",
    "#es posible notar qué palabras en español/inglés no aportan valor para el análisis (por ejemplo: cabeceras del mensaje como\n",
    "#\"mime-version\") y es necesario crear una lista personalizada con estas palabras para luego poder quitarlas del texto y \n",
    "#y obtener resultados más precisos tras la limpieza de los datos\n",
    "\n",
    "#Crear la lista de palabras\n",
    "#own_stop_words = ['fdbfaa','aaacjzptmf','aaapykc','alt','stron','solo','vale','revx','gracias','ecom','elacion','llevar','rati','ejer','ollo','caracter']\n",
    "\n",
    "#Crear un fichero para escribir en este la lista de palabras\n",
    "#with open('C:\\........\\own_stop_words.txt', 'w') as file: \n",
    " #   for item in specific_stop_words:\n",
    "  #      file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd417602",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leer la lista para quitar del texto dicho listado de palabras\n",
    "with open('C:\\\\..........\\\\specific_words.txt') as list_own:\n",
    "    own_stop_words = list_own.read()\n",
    "   # print(own_stop_words)\n",
    "\n",
    "df3['Texto'] = df3['Texto'].apply(lambda x: ' '.join([word for word in x.split() if word not in (own_stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061cebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c1fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_str4(text):\n",
    "\n",
    "    text = re.sub(r'\\b\\w{2}\\b', '', text) #quita las palabras con dos letras\n",
    "    text = re.sub(r'\\b\\w{3}\\b', '', text) #quita las palabras con tres letras\n",
    "    text = re.sub(' +', ' ', text)  #quita espacios en blanco entre palabras\n",
    "\n",
    "    return text.strip().lower()\n",
    "\n",
    "clean4 = lambda x: clean_str4(x)\n",
    "\n",
    "df4 = pd.DataFrame(df3.Texto.apply(clean4).dropna()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0eaae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convertir el corpus (con datos limpios) a pickle para luego poder analizarlo\n",
    "df4.to_pickle(\"C:\\\\....\\\\mail_clean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c098c91a",
   "metadata": {},
   "source": [
    "### Ahora creamos la matriz para contar la frecuencia de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c924b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "data_cv = cv.fit_transform(df3.Texto)\n",
    "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "data_dtm.index = df4.index\n",
    "data_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar el count vectorizer (cv)\n",
    "data_dtm.to_pickle(\"C:\\\\......\\\\dtm.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc4327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos y transponemos el document-term matrix para luego poder contar las palabras más comunes sin perder los índices\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "data = pd.read_pickle(\"C:\\\\.................\\\\email_dtm.pkl\")\n",
    "data = data.transpose()\n",
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00743d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15b2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario con los datos de la matriz transpuesta que contiene dos columnas, \n",
    "#una con las palabras y otra con su frecuencia. Con esto y utilizando .head() se obtiene el top \"n\" palabras más repetidas\n",
    "top_dict = {}\n",
    "for c in data.columns:\n",
    "    #Agregar .head(200) y la cant de primeras filas que se quiera mostrar\n",
    "    top = data[c].sort_values(ascending=False).head(400) \n",
    "    top_dict[c]= list(zip(top.index, top.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be72a7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Imprimir el diccionario\n",
    "#top_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ee9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir el diccionario creado como una lista de palabras\n",
    "for employee, top_words in top_dict.items():\n",
    "    print(employee)\n",
    "    print(', '.join([word for word, count in top_words[0:400]]))\n",
    "    print('----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842ae23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cantidad de palabras\n",
    "employees = data.columns.values\n",
    "\n",
    "unique_list = []\n",
    "for body in data.columns:\n",
    "    uniques = data[body].to_numpy().nonzero()[0].size\n",
    "    unique_list.append(uniques)\n",
    "\n",
    "# Creamos un dataframe con la cantidad de palabras únicas\n",
    "data_words = pd.DataFrame(list(zip(employees, unique_list)), columns=['Empleados','Palabras_únicas'])\n",
    "data_unique_sort = data_words.sort_values(by='Palabras_únicas')\n",
    "data_unique_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fbaf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la cantidad total de palabras utilizadas por cada empleado\n",
    "employees = data.columns.values\n",
    "\n",
    "total_list = []\n",
    "for body in data.columns:\n",
    "    total = data[body].to_numpy().sum()\n",
    "    total_list.append(total)\n",
    "\n",
    "# Create a new dataframe that contains this unique word count\n",
    "data_words = pd.DataFrame(list(zip(employees, total_list)), columns=['Empleados','Total_palabras'])\n",
    "data_total_sort = data_words.sort_values(by='Total_palabras')\n",
    "data_total_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2be0a",
   "metadata": {},
   "source": [
    "### Ahora mostramos en un gráfico wordcloud las palabras más usadas por cada empleado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "#Leer el corpus .pkl creado tras la limpieza del texto y renombrarlo como data_corpus para trabajar el wordcloud\n",
    "data_corpus = pd.read_pickle(\"C:\\\\Users\\\\....\\\\mail.pkl\")\n",
    "data_corpus.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear una lista de las palabras que contiene el texto analizado\n",
    "text_to_list = ' '.join(data_corpus['Texto'].tolist())\n",
    "#Contar la frecuencia de repetición de cada palabra que se ha agregado a la lista creada\n",
    "word_frequency = pd.Series(text_to_list.split()).value_counts()\n",
    "#word_frecuency\n",
    "top_words = word_frequency.head(400)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9a0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerías para crear un wordcloud de las 200 palabras más repetidas en el texto analizado\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "stop_words = set(STOPWORDS)\n",
    "#stop_words  #imprimir listado de palabras en inglés que contiene STOPWORDS de python\n",
    "wc = WordCloud(height= 400, width=800, background_color=\"white\",\n",
    "               stopwords=stop_words).generate_from_frequencies(top_words)\n",
    "\n",
    "# Crear el gráfico wordcloud utilizando Matplotlib y guardarlo como .png\n",
    "plt.figure(figsize=(100, 30))\n",
    "plt.tight_layout(pad=2)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('E-mail Director', fontsize=200)\n",
    "plt.savefig (\"C:\\\\....\\\\email_empleado.png\")\n",
    "#Imprimir el wordcloud en el mismo notebook\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcec05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
